environment:
    type: "NASim"
    name: "SmallGenPO-v0"
gamma: 0.99
lamda: 0.95
timesteps: 100000
updates: 10                 # Number of cycles that the entire PPO algorithm will run
epochs: 4                   # Number of times that the whole batch of data is used to update the policy 
n_workers: 16               # Number of environments that are used to sample training data
worker_steps: 512           # Number of steps an agent samples data in each environment (batch size = n_workers * worker_steps)
n_mini_batch: 4             # Number of mini-batches that are trained throughout one epoch
value_loss_coefficient: 0.2 # Multiplier of the value function loss to constrain it
hidden_layer_size: 128
max_grad_norm: 0.5          # Gradients are clippled by the specified mex norm
transformer:
    num_blocks: 4           # Number of training blocks
    embed_dim: 128          # Emdedding size of every layer inside a transformer block, this is basically the hidden state dimension
    num_heads: 4            # Number of heads in the transformer's multi-head attention mechanism
    memory_length: 32       # Length of the sliding episodic memory window
    positional_encoding: "" # options: "" "relative" "learned". If empty -> No positional encoding is used.
    layer_norm: "pre"       # options: "" "pre" "post", whether to apply layer normalization before or after every transformer component. If empty, then no layer normalization is applied
    gtrxl: True             # Whether to use the GTrXL architecture
    gtrxl_bias: 0.0         # Bias for the GTrXL architecture
learning_rate_schedule:
    initial: 3.0e-4
    final: 3.0e-5
    power: 1.0
    max_decay_steps: 10
beta_schedule:
    initial: 0.001
    final: 0.0001
    power: 1.0
    max_decay_steps: 10
clip_range_schedule:
    initial: 0.2
    final: 0.2
    power: 1.0
    max_decay_steps: 10
